<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Productivity on</title><link>/categories/productivity/</link><description>Recent content in Productivity on</description><generator>Hugo</generator><language>en</language><lastBuildDate>Sat, 07 May 2022 14:30:00 +0530</lastBuildDate><atom:link href="/categories/productivity/index.xml" rel="self" type="application/rss+xml"/><item><title>How I Saved Several Hours To Clean Data Using The Terminal Instead Of Python</title><link>/blogs/how-i-saved-several-hours-to-clean-data-using-the-terminal-instead-of-python/</link><pubDate>Sat, 07 May 2022 14:30:00 +0530</pubDate><guid>/blogs/how-i-saved-several-hours-to-clean-data-using-the-terminal-instead-of-python/</guid><description>&lt;h2 id="background"&gt;Background&lt;/h2&gt;
&lt;p&gt;Few weeks ago at work, I was given a dataset by some business guys. They wanted to find some keywords in specific text columns. The data was huge about ~300MB, approximately 3 million rows, and had about 30 columns delimited by |. Data in a few columns were repeated and as a result, it was introducing redundancy in the dataset. As the data was directly exported by a DBMS to a CSV format, there were some formatting issues as well. Not only the file was huge but the format of the data wasn&amp;rsquo;t ideal to be simply read by pandas for data manipulation. Moreover, opening this file in Excel was a challenge as only the first 1000k rows were being displayed and there was a possibility of truncation of data.&lt;/p&gt;</description></item></channel></rss>